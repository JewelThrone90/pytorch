{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Basics\n",
    "\n",
    "## Part 1: Basic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### basics ######\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# use gpu if available\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3948, -1.7226, -1.7597,  1.8367,  1.2553],\n",
       "        [ 2.8181, -0.1345,  0.1668,  1.5465,  0.1209],\n",
       "        [ 0.2747,  0.4224, -0.0367, -0.6486, -1.6042],\n",
       "        [-0.1773, -2.2585,  1.2980, -1.6444,  0.5410]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating tensors\n",
    "x = torch.empty(4,5)\n",
    "x = torch.zeros(4,5)\n",
    "x = torch.ones(4,1, dtype = torch.float32)\n",
    "x = torch.eye(4) #identity matrix\n",
    "\n",
    "x = torch.randint(low=0, high=100, size=(4,1)) # randomints\n",
    "x = torch.rand(4, 5, requires_grad=True) # random uniform\n",
    "x = torch.normal(0, 1, size =(4,5)) # random normal\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([4, 5]), dtype: torch.float32, requires_grad: False, device: cpu\n"
     ]
    }
   ],
   "source": [
    "# tensort attributes\n",
    "print(f'shape: {x.shape}, dtype: {x.dtype}, requires_grad: {x.requires_grad}, device: {x.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "x = torch.rand(4,5)\n",
    "x_numpy = x.numpy() # return a copy of x as numpy\n",
    "x = x.to(device) # return a copy of x on the device\n",
    "x_cpu = x.to('cpu') # return a copy of x on the cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_vals: tensor([1.6570, 1.6633, 1.9301, 1.9928]) indices: tensor([0, 0, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# some math methods\n",
    "x.T # transpose\n",
    "x @ x.T # matrix multiplication\n",
    "x.add(x) # add\n",
    "x.add_(x) # method ending with \"_\" is in place\n",
    "\n",
    "max_vals, max_args = x.max(dim=1) # return max vals and indices by row\n",
    "print('max_vals:', max_vals, 'indices:', max_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,0] : tensor(1.6570)\n",
      "x[0,0].item() : 1.6570290327072144\n"
     ]
    }
   ],
   "source": [
    "# getting values\n",
    "print('x[0,0] :', x[0,0]) # this is a one element tensor\n",
    "print('x[0,0].item() :', x[0,0].item()) # this a numpy float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.6613, 0.1218, 0.5651], requires_grad=True) x.grad: None\n",
      "y: tensor([2.6613, 2.1218, 2.5651], grad_fn=<AddBackward0>) x.grad: None\n",
      "z: tensor(18.1647, grad_fn=<AddBackward0>) x.grad: None\n",
      "calling z.backward()\n",
      "(dz/dx) x_grad: tensor([5.3226, 4.2437, 5.1302])\n",
      "check gradient: tensor([True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# gradients:\n",
    "# lets calc dz/dx where z is scalar z(y(x))\n",
    "# here z = sum(y**2) where y = x + 2\n",
    "x = torch.rand(3, requires_grad=True) # gradient tracked: we want dz/dx later\n",
    "print('x:', x, 'x.grad:', x.grad) # shows required grad = true\n",
    "y = x + 2  \n",
    "print('y:', y, 'x.grad:', x.grad) # y now has grad_fn attribute <AddBackward>\n",
    "z = sum(y**2)\n",
    "print('z:', z, 'x.grad:', x.grad)  \n",
    "print('calling z.backward()')\n",
    "z.backward() # dz/dx is calculated and x.grad is now populated\n",
    "print('(dz/dx) x_grad:', x.grad)\n",
    "print('check gradient:', x.grad == 2*(x+2)) # xgrad is dz/dx i.e. 2*(x+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop tracking gradients (3 ways)\n",
    "x.requires_grad_(False)\n",
    "x.detach()\n",
    "with torch.no_grad(): # wrap code in this\n",
    "    x+x\n",
    "\n",
    "# reset gradients, this is important as otherwise looping will add to the gradient\n",
    "x.grad.zero_() # do this after each training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Full Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, loss: 6.58 estimated params: w_0: 1.38 w_1: 1.64 w_2: 2.76 w_3: 3.38 w_4: 3.92 b: 8.04\n",
      "epoch: 50, loss: 0.30 estimated params: w_0: 1.17 w_1: 1.92 w_2: 3.03 w_3: 3.90 w_4: 4.72 b: 9.60\n",
      "epoch: 75, loss: 0.02 estimated params: w_0: 1.05 w_1: 1.97 w_2: 3.02 w_3: 3.99 w_4: 4.92 b: 9.92\n",
      "epoch: 100, loss: 0.00 estimated params: w_0: 1.02 w_1: 1.99 w_2: 3.01 w_3: 4.00 w_4: 4.98 b: 9.98\n",
      "epoch: 125, loss: 0.00 estimated params: w_0: 1.00 w_1: 2.00 w_2: 3.00 w_3: 4.00 w_4: 4.99 b: 10.00\n",
      "epoch: 150, loss: 0.00 estimated params: w_0: 1.00 w_1: 2.00 w_2: 3.00 w_3: 4.00 w_4: 5.00 b: 10.00\n"
     ]
    }
   ],
   "source": [
    "##### linear regression using autograd #####\n",
    "# true parameters\n",
    "w_true = torch.tensor([1,2,3,4,5], dtype = torch.float32).view(-1,1)\n",
    "b_true = torch.tensor(10, dtype = torch.float32)\n",
    "\n",
    "# X is input, y is target\n",
    "X = torch.normal(mean=0, std=1, size=(100,5)) # n = 100, dim = 5\n",
    "y = X @ w_true + b_true\n",
    "\n",
    "# define the model parameters\n",
    "w_est = torch.normal(mean=0, std=0.1, size = (5,1), dtype= torch.float32, requires_grad = True)\n",
    "b_est = torch.normal(mean=0, std=0.1, size = (1,1), dtype= torch.float32, requires_grad = True)\n",
    "\n",
    "# define the model\n",
    "def forward(X):\n",
    "    return X @ w_est + b_est\n",
    "\n",
    "# define the loss function\n",
    "def loss(y, y_pred):\n",
    "    return ((y-y_pred)**2).mean() # MSE\n",
    "\n",
    "# define the learning parameters\n",
    "learning_rate, epochs = 0.03, 150\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    #forward pass\n",
    "    y_pred = forward(X)\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    #backward pass\n",
    "    l.backward() # this calculate dl/dw and dl/db\n",
    "\n",
    "    #updates\n",
    "    with torch.no_grad(): # this calc should not recalculate the gradients\n",
    "        w_est -= learning_rate * w_est.grad\n",
    "        b_est -= learning_rate * b_est.grad\n",
    "    \n",
    "    #zeroise grads (otherwise the gradient will be incremented each loop)\n",
    "    w_est.grad.zero_(), b_est.grad.zero_() \n",
    "\n",
    "    #print output\n",
    "    if((epoch + 1) %25 == 0): # console update every 20 iterations\n",
    "        param_string = ' '.join([f'w_{i}: {w.item():.2f}' for i, w in enumerate(w_est)]) + f' b: {b_est.item():.2f}'\n",
    "        print(f'epoch: {epoch + 1}, loss: {l:.2f}', 'estimated params:', param_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.11, train_acc:0.97, test_acc:0.96\n",
      "epoch: 40, loss: 0.09, train_acc:0.98, test_acc:0.97\n",
      "epoch: 60, loss: 0.08, train_acc:0.98, test_acc:0.97\n",
      "epoch: 80, loss: 0.07, train_acc:0.98, test_acc:0.97\n",
      "epoch: 100, loss: 0.07, train_acc:0.98, test_acc:0.97\n"
     ]
    }
   ],
   "source": [
    "##### logistic regression using pytorch class structure #####\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 0) prepare data\n",
    "dataset = load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "n_samples, n_features = X.shape \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, stratify = y, random_state = 123)\n",
    "\n",
    "# standardise data and convert to tensors\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1,1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "def calc_accuracy(y_prob, y_true):\n",
    "    y_pred = y_prob.round() # round to 0 or 1\n",
    "    acc = (y_pred == y_true).sum()/len(y_true)\n",
    "    return acc\n",
    "\n",
    "# 1) model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        # initialze superclass\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # initialize layer objects\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = torch.sigmoid(self.linear(X))\n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(n_features).to(device)\n",
    "\n",
    "# 2) loss and optimizer\n",
    "criterion = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    #forward pass\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    y_prob = model(X_train)\n",
    "    loss = criterion(y_prob, y_train)\n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    #updates\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if((epoch +1) % 20 == 0):\n",
    "        with torch.no_grad(): # we don't want to add to the gradients, having just zeroised them\n",
    "            X_test.to(device), y_test.to(device)\n",
    "            train_acc = calc_accuracy(y_prob, y_train)\n",
    "            test_acc = calc_accuracy(model(X_test), y_test)\n",
    "            print(f'epoch: {epoch +1}, loss: {loss.item():.2f}, train_acc:{train_acc:.2f}, test_acc:{test_acc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minas-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
